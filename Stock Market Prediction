Stock Market Prediction And Forecasting Using Stacked LSTM
Stock market prediction and forecasting using LSTM by Nazia Zehan
### Keras and Tensorflow >2.0
### Data Collection
import pandas_datareader as pdr
key=""
df = pdr.get_data_tiingo('AAPL', api_key=key)
df.to_csv('AAPL.csv')
import pandas as pd
df=pd.read_csv('AAPL.csv')
df.head()
Unnamed: 0	symbol	date	close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
0	0	AAPL	2015-05-27 00:00:00+00:00	132.045	132.260	130.05	130.34	45833246	121.682558	121.880685	119.844118	120.111360	45833246	0.0	1.0
1	1	AAPL	2015-05-28 00:00:00+00:00	131.780	131.950	131.10	131.86	30733309	121.438354	121.595013	120.811718	121.512076	30733309	0.0	1.0
2	2	AAPL	2015-05-29 00:00:00+00:00	130.280	131.450	129.90	131.23	50884452	120.056069	121.134251	119.705890	120.931516	50884452	0.0	1.0
3	3	AAPL	2015-06-01 00:00:00+00:00	130.535	131.390	130.05	131.20	32112797	120.291057	121.078960	119.844118	120.903870	32112797	0.0	1.0
4	4	AAPL	2015-06-02 00:00:00+00:00	129.960	130.655	129.32	129.86	33667627	119.761181	120.401640	119.171406	119.669029	33667627	0.0	1.0
df.tail()
Unnamed: 0	symbol	date	close	high	low	open	volume	adjClose	adjHigh	adjLow	adjOpen	adjVolume	divCash	splitFactor
1253	1253	AAPL	2020-05-18 00:00:00+00:00	314.96	316.50	310.3241	313.17	33843125	314.96	316.50	310.3241	313.17	33843125	0.0	1.0
1254	1254	AAPL	2020-05-19 00:00:00+00:00	313.14	318.52	313.0100	315.03	25432385	313.14	318.52	313.0100	315.03	25432385	0.0	1.0
1255	1255	AAPL	2020-05-20 00:00:00+00:00	319.23	319.52	316.2000	316.68	27876215	319.23	319.52	316.2000	316.68	27876215	0.0	1.0
1256	1256	AAPL	2020-05-21 00:00:00+00:00	316.85	320.89	315.8700	318.66	25672211	316.85	320.89	315.8700	318.66	25672211	0.0	1.0
1257	1257	AAPL	2020-05-22 00:00:00+00:00	318.89	319.23	315.3500	315.77	20450754	318.89	319.23	315.3500	315.77	20450754	0.0	1.0
df1=df.reset_index()['close']
df1
0       132.045
1       131.780
2       130.280
3       130.535
4       129.960
         ...   
1253    314.960
1254    313.140
1255    319.230
1256    316.850
1257    318.890
Name: close, Length: 1258, dtype: float64
import matplotlib.pyplot as plt
plt.plot(df1)
[<matplotlib.lines.Line2D at 0x2d1a92724e0>]

### LSTM are sensitive to the scale of the data. so we apply MinMax scaler 
import numpy as np
df1
0       132.045
1       131.780
2       130.280
3       130.535
4       129.960
         ...   
1253    314.960
1254    313.140
1255    319.230
1256    316.850
1257    318.890
Name: close, Length: 1258, dtype: float64
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler(feature_range=(0,1))
df1=scaler.fit_transform(np.array(df1).reshape(-1,1))
print(df1)
[[0.17607447]
 [0.17495567]
 [0.16862282]
 ...
 [0.96635143]
 [0.9563033 ]
 [0.96491598]]
##splitting dataset into train and test split
training_size=int(len(df1)*0.65)
test_size=len(df1)-training_size
train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]
training_size,test_size
(817, 441)
train_data
array([[0.17607447],
       [0.17495567],
       [0.16862282],
       [0.1696994 ],
       [0.16727181],
       [0.16794731],
       [0.16473866],
       [0.16174111],
       [0.1581525 ],
       [0.15654817],
       [0.16271215],
       [0.1614878 ],
       [0.1554927 ],
       [0.15443722],
       [0.15730811],
       [0.15604154],
       [0.15849025],
       [0.15308621],
       [0.15735033],
       [0.15490163],
       [0.15946129],
       [0.15688592],
       [0.1537195 ],
       [0.14434687],
       [0.14812547],
       [0.15308621],
       [0.15241071],
       [0.15055307],
       [0.14924428],
       [0.13607194],
       [0.12551718],
       [0.13906949],
       [0.14911762],
       [0.14890653],
       [0.15401503],
       [0.16115005],
       [0.16583636],
       [0.17618002],
       [0.17060711],
       [0.14725998],
       [0.14700667],
       [0.14422021],
       [0.13691632],
       [0.13949168],
       [0.13784514],
       [0.13522756],
       [0.13071012],
       [0.11863548],
       [0.10259225],
       [0.1058009 ],
       [0.10466098],
       [0.10630752],
       [0.12403952],
       [0.09773706],
       [0.10512539],
       [0.10474542],
       [0.10816516],
       [0.11323144],
       [0.11044499],
       [0.10415435],
       [0.09419066],
       [0.06510175],
       [0.05395592],
       [0.0565735 ],
       [0.08169383],
       [0.09533058],
       [0.09689268],
       [0.09465507],
       [0.07337668],
       [0.09288187],
       [0.08456472],
       [0.07992063],
       [0.09275521],
       [0.0836359 ],
       [0.09385291],
       [0.10077683],
       [0.10542092],
       [0.10951617],
       [0.11006502],
       [0.09955248],
       [0.09756818],
       [0.10499873],
       [0.09735709],
       [0.10124124],
       [0.10411213],
       [0.10288778],
       [0.09330406],
       [0.07903403],
       [0.08426919],
       [0.08122942],
       [0.08460694],
       [0.0862957 ],
       [0.08853331],
       [0.0862957 ],
       [0.08089167],
       [0.09195305],
       [0.08975766],
       [0.09055982],
       [0.08388922],
       [0.09085536],
       [0.0873934 ],
       [0.09030651],
       [0.09891919],
       [0.09887697],
       [0.10622309],
       [0.1213375 ],
       [0.10529427],
       [0.10221228],
       [0.12213966],
       [0.12745926],
       [0.1231107 ],
       [0.1302035 ],
       [0.13607194],
       [0.13366546],
       [0.1291058 ],
       [0.12969687],
       [0.12762813],
       [0.1115849 ],
       [0.10879845],
       [0.1071519 ],
       [0.09288187],
       [0.10062906],
       [0.09858144],
       [0.11378029],
       [0.12007093],
       [0.12226632],
       [0.11572237],
       [0.12049312],
       [0.1169045 ],
       [0.11597568],
       [0.11804441],
       [0.11399139],
       [0.10951617],
       [0.10495651],
       [0.1211264 ],
       [0.11795998],
       [0.11774888],
       [0.10672971],
       [0.10905176],
       [0.09642827],
       [0.09347294],
       [0.08507135],
       [0.08865997],
       [0.07869628],
       [0.06624166],
       [0.07173014],
       [0.07130795],
       [0.07713417],
       [0.07468547],
       [0.06957697],
       [0.07768302],
       [0.07168792],
       [0.0629908 ],
       [0.06337077],
       [0.05222494],
       [0.04373892],
       [0.02579583],
       [0.027949  ],
       [0.03457739],
       [0.04061471],
       [0.02976442],
       [0.03875707],
       [0.02866672],
       [0.02668243],
       [0.02723128],
       [0.02516254],
       [0.04677869],
       [0.03841932],
       [0.04074137],
       [0.01300346],
       [0.01583214],
       [0.02955332],
       [0.02571139],
       [0.01747868],
       [0.02537364],
       [0.02642911],
       [0.0155366 ],
       [0.01971629],
       [0.01963185],
       [0.01659208],
       [0.01418559],
       [0.01540995],
       [0.02659799],
       [0.03284641],
       [0.02499367],
       [0.02406485],
       [0.02761125],
       [0.01836528],
       [0.02431816],
       [0.02710462],
       [0.0277379 ],
       [0.02680909],
       [0.04302119],
       [0.04395001],
       [0.04711644],
       [0.05349151],
       [0.04867854],
       [0.04513215],
       [0.04551212],
       [0.04572321],
       [0.05032509],
       [0.05142278],
       [0.0601199 ],
       [0.06598835],
       [0.06527062],
       [0.06577725],
       [0.06573503],
       [0.06915477],
       [0.06666385],
       [0.06472178],
       [0.06269526],
       [0.0732078 ],
       [0.08114498],
       [0.0787385 ],
       [0.0829604 ],
       [0.08773115],
       [0.08220046],
       [0.08705564],
       [0.07683864],
       [0.07734527],
       [0.07886515],
       [0.08486026],
       [0.0916153 ],
       [0.09186861],
       [0.08236933],
       [0.07236342],
       [0.06995694],
       [0.07088576],
       [0.06598835],
       [0.064764  ],
       [0.06223085],
       [0.05914886],
       [0.03157984],
       [0.01895635],
       [0.01435447],
       [0.01393228],
       [0.02043401],
       [0.01625433],
       [0.01224352],
       [0.01004813],
       [0.01034366],
       [0.01300346],
       [0.00916153],
       [0.        ],
       [0.00075994],
       [0.01494554],
       [0.013299  ],
       [0.01781643],
       [0.01629655],
       [0.02060289],
       [0.02571139],
       [0.03191759],
       [0.03917926],
       [0.04251457],
       [0.04226125],
       [0.04019252],
       [0.03428185],
       [0.03115765],
       [0.03200203],
       [0.03499958],
       [0.03668834],
       [0.03630837],
       [0.03930592],
       [0.03584396],
       [0.02955332],
       [0.03005995],
       [0.02870894],
       [0.03043992],
       [0.0210673 ],
       [0.02009626],
       [0.023516  ],
       [0.02199612],
       [0.02431816],
       [0.01291902],
       [0.00717724],
       [0.01372119],
       [0.01714093],
       [0.02220721],
       [0.02343156],
       [0.01963185],
       [0.02191168],
       [0.02364266],
       [0.02676687],
       [0.02803344],
       [0.02989107],
       [0.02756903],
       [0.03567508],
       [0.03563286],
       [0.04006586],
       [0.04023474],
       [0.04061471],
       [0.0383771 ],
       [0.03512623],
       [0.02955332],
       [0.02672465],
       [0.0532382 ],
       [0.05910665],
       [0.0585578 ],
       [0.0663261 ],
       [0.05969771],
       [0.0652284 ],
       [0.06556616],
       [0.07236342],
       [0.07612092],
       [0.07797855],
       [0.07455881],
       [0.07426328],
       [0.07531875],
       [0.08080723],
       [0.08038504],
       [0.07970953],
       [0.07911847],
       [0.0803006 ],
       [0.07671198],
       [0.07814743],
       [0.07468547],
       [0.07274339],
       [0.07008359],
       [0.06957697],
       [0.066115  ],
       [0.06653719],
       [0.06919699],
       [0.0734189 ],
       [0.07329224],
       [0.0760787 ],
       [0.06408849],
       [0.05399814],
       [0.06375074],
       [0.07434772],
       [0.09047539],
       [0.10651862],
       [0.10377438],
       [0.09811703],
       [0.09807481],
       [0.09799037],
       [0.10250781],
       [0.09444398],
       [0.0951617 ],
       [0.0960483 ],
       [0.09967914],
       [0.09220637],
       [0.09587942],
       [0.09364181],
       [0.09566833],
       [0.09587942],
       [0.09942582],
       [0.10014354],
       [0.10854513],
       [0.10960061],
       [0.11399139],
       [0.1124715 ],
       [0.11521574],
       [0.11487799],
       [0.11454023],
       [0.11306257],
       [0.11280925],
       [0.11086718],
       [0.11530018],
       [0.11783332],
       [0.10660306],
       [0.10191674],
       [0.0987081 ],
       [0.09794816],
       [0.08929325],
       [0.08971544],
       [0.08228489],
       [0.07810521],
       [0.0847336 ],
       [0.08747784],
       [0.08671789],
       [0.07367221],
       [0.07637423],
       [0.06489065],
       [0.07080132],
       [0.0829604 ],
       [0.08279152],
       [0.08325593],
       [0.09030651],
       [0.09060204],
       [0.08819556],
       [0.09055982],
       [0.08963101],
       [0.0891666 ],
       [0.08519801],
       [0.08084945],
       [0.08258043],
       [0.07924512],
       [0.08279152],
       [0.08735118],
       [0.09195305],
       [0.09967914],
       [0.0969349 ],
       [0.1049143 ],
       [0.1049143 ],
       [0.10757409],
       [0.10820738],
       [0.11103606],
       [0.11234485],
       [0.11280925],
       [0.10955839],
       [0.11052943],
       [0.11365364],
       [0.11154268],
       [0.11141603],
       [0.10757409],
       [0.10896732],
       [0.10841848],
       [0.1109094 ],
       [0.11639787],
       [0.12095753],
       [0.12146416],
       [0.12416617],
       [0.12205522],
       [0.12116862],
       [0.12522165],
       [0.12517943],
       [0.12429283],
       [0.12522165],
       [0.1255594 ],
       [0.12509499],
       [0.13315883],
       [0.13341214],
       [0.13345436],
       [0.13210335],
       [0.13092122],
       [0.1621633 ],
       [0.16123448],
       [0.16355653],
       [0.16866503],
       [0.17390019],
       [0.17605336],
       [0.17765769],
       [0.17639112],
       [0.18133074],
       [0.18863464],
       [0.19070337],
       [0.19000676],
       [0.19158997],
       [0.19572743],
       [0.19745841],
       [0.19500971],
       [0.19555856],
       [0.19669847],
       [0.19695179],
       [0.20877311],
       [0.20526894],
       [0.2087309 ],
       [0.20687326],
       [0.2076332 ],
       [0.20543781],
       [0.2040868 ],
       [0.20602888],
       [0.20628219],
       [0.20539559],
       [0.21160179],
       [0.21257283],
       [0.2096175 ],
       [0.21582369],
       [0.20898421],
       [0.21565482],
       [0.21354387],
       [0.21236173],
       [0.21337499],
       [0.22570295],
       [0.22705396],
       [0.22625179],
       [0.22511188],
       [0.22528076],
       [0.22979819],
       [0.22663177],
       [0.22511188],
       [0.22376087],
       [0.22304315],
       [0.21654142],
       [0.21725914],
       [0.21409271],
       [0.2173858 ],
       [0.214726  ],
       [0.21253061],
       [0.21996116],
       [0.21924343],
       [0.22502744],
       [0.22878494],
       [0.22519632],
       [0.22566073],
       [0.22506966],
       [0.23743984],
       [0.24136621],
       [0.23946635],
       [0.23722874],
       [0.24748797],
       [0.26458668],
       [0.26872414],
       [0.26564215],
       [0.26855526],
       [0.27763236],
       [0.2759436 ],
       [0.27497256],
       [0.25293422],
       [0.26260238],
       [0.26479777],
       [0.26872414],
       [0.26792198],
       [0.2659799 ],
       [0.26821751],
       [0.26711982],
       [0.26737313],
       [0.2635312 ],
       [0.2653044 ],
       [0.27488812],
       [0.26847083],
       [0.27066622],
       [0.27455037],
       [0.27294604],
       [0.24757241],
       [0.23254243],
       [0.23748206],
       [0.23144474],
       [0.22777168],
       [0.21924343],
       [0.23642658],
       [0.23081145],
       [0.23444229],
       [0.23342903],
       [0.23617327],
       [0.23423119],
       [0.22540741],
       [0.23427341],
       [0.22519632],
       [0.22663177],
       [0.22443638],
       [0.2269273 ],
       [0.22118551],
       [0.22730727],
       [0.23102254],
       [0.23300684],
       [0.23389344],
       [0.2424639 ],
       [0.24782572],
       [0.25002111],
       [0.2522165 ],
       [0.25618509],
       [0.25331419],
       [0.25301866],
       [0.26070252],
       [0.26344676],
       [0.26648653],
       [0.25424301],
       [0.2497678 ],
       [0.24651693],
       [0.25208984],
       [0.28202314],
       [0.27539475],
       [0.27885671],
       [0.28907371],
       [0.29443553],
       [0.298573  ],
       [0.27433927],
       [0.28345858],
       [0.29346449],
       [0.30085282],
       [0.29810859],
       [0.28506291],
       [0.28354302],
       [0.28231867],
       [0.29316896],
       [0.29401334],
       [0.29101579],
       [0.29350671],
       [0.30030398],
       [0.30638352],
       [0.30824116],
       [0.31098539],
       [0.31119649],
       [0.30287934],
       [0.30216161],
       [0.29941738],
       [0.28831377],
       [0.30043063],
       [0.29772862],
       [0.29262011],
       [0.28683611],
       [0.29359115],
       [0.28848265],
       [0.28873596],
       [0.2775057 ],
       [0.266191  ],
       [0.25985814],
       [0.25420079],
       [0.26513552],
       [0.2697374 ],
       [0.26572659],
       [0.26927299],
       [0.2679642 ],
       [0.27079287],
       [0.26657097],
       [0.27463481],
       [0.27425483],
       [0.27653466],
       [0.27678798],
       [0.27953221],
       [0.27721017],
       [0.28138985],
       [0.29359115],
       [0.29608207],
       [0.29308452],
       [0.27712573],
       [0.27826564],
       [0.27792789],
       [0.28185426],
       [0.27894115],
       [0.28316305],
       [0.30697458],
       [0.32246897],
       [0.33226378],
       [0.32318669],
       [0.32833741],
       [0.34687157],
       [0.3542599 ],
       [0.35662417],
       [0.36266149],
       [0.3611416 ],
       [0.3560331 ],
       [0.35307777],
       [0.34197416],
       [0.33243266],
       [0.34096091],
       [0.3369501 ],
       [0.33623237],
       [0.34957359],
       [0.35725745],
       [0.35729967],
       [0.3535844 ],
       [0.34927805],
       [0.33412142],
       [0.34412733],
       [0.34074981],
       [0.33547243],
       [0.33479693],
       [0.33213713],
       [0.33344592],
       [0.33365701],
       [0.34758929],
       [0.34349405],
       [0.34590053],
       [0.34568944],
       [0.35307777],
       [0.36342143],
       [0.35548425],
       [0.35468209],
       [0.35746855],
       [0.35746855],
       [0.3387233 ],
       [0.33884995],
       [0.34087647],
       [0.33306595],
       [0.34585831],
       [0.34573166],
       [0.34910918],
       [0.35742633],
       [0.35468209],
       [0.35459765],
       [0.35442878],
       [0.35860846],
       [0.36625011],
       [0.36245039],
       [0.37473613],
       [0.37541164],
       [0.37203411],
       [0.36587013],
       [0.36603901],
       [0.35413324],
       [0.34100312],
       [0.34269189],
       [0.32770413],
       [0.32352444],
       [0.32546652],
       [0.32694419],
       [0.29620873],
       [0.2792789 ],
       [0.30689015],
       [0.2921557 ],
       [0.27362155],
       [0.27894115],
       [0.30553914],
       [0.31242084],
       [0.32521321],
       [0.3489403 ],
       [0.34657604],
       [0.34412733],
       [0.34083425],
       [0.34687157],
       [0.35953728],
       [0.37418728],
       [0.37173858],
       [0.37059867],
       [0.35742633],
       [0.36253483],
       [0.36511019],
       [0.36447691],
       [0.35755298],
       [0.36561682],
       [0.37845141],
       [0.38579752],
       [0.37840919],
       [0.37194967],
       [0.37283627],
       [0.37017648],
       [0.3586929 ],
       [0.35843958],
       [0.34167863],
       [0.33146162],
       [0.31495398],
       [0.34801148],
       [0.32930845],
       [0.32145571],
       [0.32694419],
       [0.32230009],
       [0.32951955],
       [0.34311408],
       [0.34813814],
       [0.32947733],
       [0.33652791],
       [0.350038  ],
       [0.34661826],
       [0.35379549],
       [0.35628641],
       [0.36088829],
       [0.37110529],
       [0.36941653],
       [0.34813814],
       [0.31824707],
       [0.31622055],
       [0.30651017],
       [0.30950773],
       [0.31191421],
       [0.30389259],
       [0.31630499],
       [0.3325171 ],
       [0.36405472],
       [0.36540572],
       [0.39470573],
       [0.40032086],
       [0.40407836],
       [0.40960905],
       [0.42092375],
       [0.41480199],
       [0.41294436],
       [0.4057249 ],
       [0.41307101],
       [0.40804695],
       [0.40517605],
       [0.41074897],
       [0.40876467],
       [0.41383095],
       [0.41294436],
       [0.41475977],
       [0.41188888],
       [0.41020012],
       [0.40754032],
       [0.42176813],
       [0.42848096],
       [0.43472938],
       [0.43755805],
       [0.43536266],
       [0.42793211],
       [0.42594782],
       [0.43038082],
       [0.42371021],
       [0.4241324 ],
       [0.41585747],
       [0.41543528],
       [0.40255847],
       [0.40597821],
       [0.40158744],
       [0.39930761],
       [0.38769737],
       [0.39723888],
       [0.39609896],
       [0.40175631],
       [0.40010977],
       [0.40884911],
       [0.3950857 ],
       [0.40133412],
       [0.41218441],
       [0.42320358],
       [0.42223254],
       [0.41180444],
       [0.42510344],
       [0.42637001],
       [0.42459681],
       [0.42687664],
       [0.42244364],
       [0.42869205],
       [0.42683442],
       [0.42755214],
       [0.43342059],
       [0.44110445],
       [0.43852909],
       [0.42489234],
       [0.42037491],
       [0.42197923],
       [0.46930676],
       [0.49417377],
       [0.49670692],
       [0.50126657],
       [0.49299164],
       [0.49358271],
       [0.50046441],
       [0.49476484],
       [0.50042219],
       [0.50413747],
       [0.5062062 ],
       [0.51920966],
       [0.53719497],
       [0.52824453],
       [0.52647133]])
import numpy
# convert an array of values into a dataset matrix
def create_dataset(dataset, time_step=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-time_step-1):
		a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
		dataX.append(a)
		dataY.append(dataset[i + time_step, 0])
	return numpy.array(dataX), numpy.array(dataY)
# reshape into X=t,t+1,t+2,t+3 and Y=t+4
time_step = 100
X_train, y_train = create_dataset(train_data, time_step)
X_test, ytest = create_dataset(test_data, time_step)
print(X_train.shape), print(y_train.shape)
(716, 100)
(716,)
(None, None)
print(X_test.shape), print(ytest.shape)
(340, 100)
(340,)
(None, None)
# reshape input to be [samples, time steps, features] which is required for LSTM
X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)
### Create the Stacked LSTM model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
model=Sequential()
model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))
model.add(LSTM(50,return_sequences=True))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
model.summary()
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_7 (LSTM)                (None, 100, 50)           10400     
_________________________________________________________________
lstm_8 (LSTM)                (None, 100, 50)           20200     
_________________________________________________________________
lstm_9 (LSTM)                (None, 50)                20200     
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 51        
=================================================================
Total params: 50,851
Trainable params: 50,851
Non-trainable params: 0
_________________________________________________________________
model.summary()
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_4 (LSTM)                (None, 100, 50)           10400     
_________________________________________________________________
lstm_5 (LSTM)                (None, 100, 50)           20200     
_________________________________________________________________
lstm_6 (LSTM)                (None, 50)                20200     
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 51        
=================================================================
Total params: 50,851
Trainable params: 50,851
Non-trainable params: 0
_________________________________________________________________
model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=100,batch_size=64,verbose=1)
Epoch 1/100
12/12 [==============================] - 6s 487ms/step - loss: 0.0206 - val_loss: 0.0505
Epoch 2/100
12/12 [==============================] - 4s 309ms/step - loss: 0.0035 - val_loss: 0.0046
Epoch 3/100
12/12 [==============================] - 4s 300ms/step - loss: 0.0014 - val_loss: 0.0040
Epoch 4/100
12/12 [==============================] - 3s 287ms/step - loss: 8.1361e-04 - val_loss: 0.0073
Epoch 5/100
12/12 [==============================] - 3s 290ms/step - loss: 6.6860e-04 - val_loss: 0.0062
Epoch 6/100
12/12 [==============================] - 3s 255ms/step - loss: 6.4653e-04 - val_loss: 0.0062
Epoch 7/100
12/12 [==============================] - 3s 291ms/step - loss: 6.6186e-04 - val_loss: 0.0062
Epoch 8/100
12/12 [==============================] - 4s 300ms/step - loss: 6.2498e-04 - val_loss: 0.0049
Epoch 9/100
12/12 [==============================] - 4s 297ms/step - loss: 6.2745e-04 - val_loss: 0.0042
Epoch 10/100
12/12 [==============================] - 4s 303ms/step - loss: 6.0206e-04 - val_loss: 0.0050
Epoch 11/100
12/12 [==============================] - 4s 298ms/step - loss: 5.9884e-04 - val_loss: 0.0061
Epoch 12/100
12/12 [==============================] - 4s 304ms/step - loss: 6.1458e-04 - val_loss: 0.0044
Epoch 13/100
12/12 [==============================] - 4s 304ms/step - loss: 5.6830e-04 - val_loss: 0.0041
Epoch 14/100
12/12 [==============================] - 3s 262ms/step - loss: 5.5734e-04 - val_loss: 0.0038
Epoch 15/100
12/12 [==============================] - 3s 244ms/step - loss: 5.5456e-04 - val_loss: 0.0034
Epoch 16/100
12/12 [==============================] - 3s 277ms/step - loss: 5.3865e-04 - val_loss: 0.0034
Epoch 17/100
12/12 [==============================] - 3s 271ms/step - loss: 5.3872e-04 - val_loss: 0.0032
Epoch 18/100
12/12 [==============================] - 3s 260ms/step - loss: 5.2315e-04 - val_loss: 0.0030
Epoch 19/100
12/12 [==============================] - 3s 275ms/step - loss: 5.1791e-04 - val_loss: 0.0029
Epoch 20/100
12/12 [==============================] - 3s 274ms/step - loss: 5.0077e-04 - val_loss: 0.0028
Epoch 21/100
12/12 [==============================] - 3s 273ms/step - loss: 4.8672e-04 - val_loss: 0.0032
Epoch 22/100
12/12 [==============================] - 3s 270ms/step - loss: 4.9148e-04 - val_loss: 0.0026
Epoch 23/100
12/12 [==============================] - 3s 283ms/step - loss: 4.9279e-04 - val_loss: 0.0026
Epoch 24/100
12/12 [==============================] - 4s 308ms/step - loss: 5.2013e-04 - val_loss: 0.0024
Epoch 25/100
12/12 [==============================] - 3s 275ms/step - loss: 5.7301e-04 - val_loss: 0.0024
Epoch 26/100
12/12 [==============================] - 4s 295ms/step - loss: 5.5014e-04 - val_loss: 0.0030
Epoch 27/100
12/12 [==============================] - 4s 301ms/step - loss: 4.8608e-04 - val_loss: 0.0022
Epoch 28/100
12/12 [==============================] - 3s 278ms/step - loss: 4.4525e-04 - val_loss: 0.0022
Epoch 29/100
12/12 [==============================] - 4s 299ms/step - loss: 4.2446e-04 - val_loss: 0.0028
Epoch 30/100
12/12 [==============================] - 4s 302ms/step - loss: 4.9896e-04 - val_loss: 0.0023
Epoch 31/100
12/12 [==============================] - 3s 278ms/step - loss: 4.7568e-04 - val_loss: 0.0022
Epoch 32/100
12/12 [==============================] - 4s 294ms/step - loss: 4.3184e-04 - val_loss: 0.0027
Epoch 33/100
12/12 [==============================] - 4s 292ms/step - loss: 4.1365e-04 - val_loss: 0.0025
Epoch 34/100
12/12 [==============================] - 3s 276ms/step - loss: 4.0967e-04 - val_loss: 0.0022
Epoch 35/100
12/12 [==============================] - 3s 250ms/step - loss: 3.9084e-04 - val_loss: 0.0018
Epoch 36/100
12/12 [==============================] - 3s 291ms/step - loss: 3.8744e-04 - val_loss: 0.0016
Epoch 37/100
12/12 [==============================] - 3s 254ms/step - loss: 3.6441e-04 - val_loss: 0.0024
Epoch 38/100
12/12 [==============================] - 3s 272ms/step - loss: 4.3088e-04 - val_loss: 0.0025
Epoch 39/100
12/12 [==============================] - 3s 259ms/step - loss: 4.1398e-04 - val_loss: 0.0016
Epoch 40/100
12/12 [==============================] - 3s 274ms/step - loss: 3.8981e-04 - val_loss: 0.0016
Epoch 41/100
12/12 [==============================] - 3s 261ms/step - loss: 3.4896e-04 - val_loss: 0.0028
Epoch 42/100
12/12 [==============================] - 3s 282ms/step - loss: 3.7910e-04 - val_loss: 0.0014
Epoch 43/100
12/12 [==============================] - 3s 274ms/step - loss: 3.6404e-04 - val_loss: 0.0022
Epoch 44/100
12/12 [==============================] - 3s 277ms/step - loss: 3.8073e-04 - val_loss: 0.0014
Epoch 45/100
12/12 [==============================] - 3s 276ms/step - loss: 4.0008e-04 - val_loss: 0.0016
Epoch 46/100
12/12 [==============================] - 3s 273ms/step - loss: 4.0253e-04 - val_loss: 0.0015
Epoch 47/100
12/12 [==============================] - 3s 286ms/step - loss: 3.5930e-04 - val_loss: 0.0018
Epoch 48/100
12/12 [==============================] - 3s 264ms/step - loss: 3.0690e-04 - val_loss: 0.0016
Epoch 49/100
12/12 [==============================] - 3s 288ms/step - loss: 3.0504e-04 - val_loss: 0.0022
Epoch 50/100
12/12 [==============================] - 3s 277ms/step - loss: 3.1205e-04 - val_loss: 0.0016
Epoch 51/100
12/12 [==============================] - 3s 291ms/step - loss: 2.8386e-04 - val_loss: 0.0014
Epoch 52/100
12/12 [==============================] - 3s 282ms/step - loss: 2.9832e-04 - val_loss: 0.0016
Epoch 53/100
12/12 [==============================] - 3s 287ms/step - loss: 2.8287e-04 - val_loss: 0.0018
Epoch 54/100
12/12 [==============================] - 3s 286ms/step - loss: 2.8193e-04 - val_loss: 0.0013
Epoch 55/100
12/12 [==============================] - 4s 295ms/step - loss: 2.8989e-04 - val_loss: 0.0026
Epoch 56/100
12/12 [==============================] - 3s 262ms/step - loss: 2.7761e-04 - val_loss: 0.0014
Epoch 57/100
12/12 [==============================] - 3s 270ms/step - loss: 2.6088e-04 - val_loss: 0.0016
Epoch 58/100
12/12 [==============================] - 3s 289ms/step - loss: 2.7300e-04 - val_loss: 0.0013
Epoch 59/100
12/12 [==============================] - 3s 288ms/step - loss: 2.6058e-04 - val_loss: 0.0020
Epoch 60/100
12/12 [==============================] - 3s 285ms/step - loss: 2.5682e-04 - val_loss: 0.0014
Epoch 61/100
12/12 [==============================] - 3s 285ms/step - loss: 2.4091e-04 - val_loss: 0.0013
Epoch 62/100
12/12 [==============================] - 4s 296ms/step - loss: 2.2724e-04 - val_loss: 0.0016
Epoch 63/100
12/12 [==============================] - 3s 258ms/step - loss: 2.3206e-04 - val_loss: 0.0012
Epoch 64/100
12/12 [==============================] - 3s 277ms/step - loss: 2.4468e-04 - val_loss: 0.0014
Epoch 65/100
12/12 [==============================] - 3s 266ms/step - loss: 2.2395e-04 - val_loss: 0.0012
Epoch 66/100
12/12 [==============================] - 3s 263ms/step - loss: 2.1142e-04 - val_loss: 0.0012
Epoch 67/100
12/12 [==============================] - 3s 281ms/step - loss: 2.0540e-04 - val_loss: 0.0016
Epoch 68/100
12/12 [==============================] - 4s 297ms/step - loss: 2.0560e-04 - val_loss: 0.0012
Epoch 69/100
12/12 [==============================] - 3s 218ms/step - loss: 1.9982e-04 - val_loss: 0.0014
Epoch 70/100
12/12 [==============================] - 3s 257ms/step - loss: 2.3622e-04 - val_loss: 0.0015
Epoch 71/100
12/12 [==============================] - 3s 283ms/step - loss: 2.6216e-04 - val_loss: 0.0012
Epoch 72/100
12/12 [==============================] - 3s 282ms/step - loss: 2.4869e-04 - val_loss: 0.0017
Epoch 73/100
12/12 [==============================] - 3s 280ms/step - loss: 2.1853e-04 - val_loss: 0.0013
Epoch 74/100
12/12 [==============================] - 3s 244ms/step - loss: 2.2121e-04 - val_loss: 0.0014
Epoch 75/100
12/12 [==============================] - 3s 283ms/step - loss: 1.9690e-04 - val_loss: 0.0011
Epoch 76/100
12/12 [==============================] - 3s 261ms/step - loss: 2.2144e-04 - val_loss: 0.0011
Epoch 77/100
12/12 [==============================] - 3s 282ms/step - loss: 1.8420e-04 - val_loss: 0.0011
Epoch 78/100
12/12 [==============================] - 3s 282ms/step - loss: 1.7841e-04 - val_loss: 0.0014
Epoch 79/100
12/12 [==============================] - 3s 260ms/step - loss: 1.9611e-04 - val_loss: 0.0013
Epoch 80/100
12/12 [==============================] - 3s 281ms/step - loss: 2.0224e-04 - val_loss: 0.0012
Epoch 81/100
12/12 [==============================] - 3s 290ms/step - loss: 2.1049e-04 - val_loss: 0.0020
Epoch 82/100
12/12 [==============================] - 3s 288ms/step - loss: 1.9466e-04 - val_loss: 0.0010
Epoch 83/100
12/12 [==============================] - 3s 284ms/step - loss: 1.5801e-04 - val_loss: 0.0010
Epoch 84/100
12/12 [==============================] - 3s 272ms/step - loss: 1.6260e-04 - val_loss: 9.4397e-04
Epoch 85/100
12/12 [==============================] - 3s 249ms/step - loss: 1.5695e-04 - val_loss: 0.0013
Epoch 86/100
12/12 [==============================] - 3s 242ms/step - loss: 2.0192e-04 - val_loss: 9.7445e-04
Epoch 87/100
12/12 [==============================] - 3s 271ms/step - loss: 2.2179e-04 - val_loss: 0.0020
Epoch 88/100
12/12 [==============================] - 3s 249ms/step - loss: 2.5509e-04 - val_loss: 0.0015
Epoch 89/100
12/12 [==============================] - 3s 261ms/step - loss: 1.9912e-04 - val_loss: 0.0011
Epoch 90/100
12/12 [==============================] - 3s 265ms/step - loss: 1.6930e-04 - val_loss: 8.9285e-04
Epoch 91/100
12/12 [==============================] - 3s 276ms/step - loss: 1.6435e-04 - val_loss: 9.1264e-04
Epoch 92/100
12/12 [==============================] - 3s 259ms/step - loss: 1.6799e-04 - val_loss: 0.0014
Epoch 93/100
12/12 [==============================] - 3s 282ms/step - loss: 1.9593e-04 - val_loss: 0.0016
Epoch 94/100
12/12 [==============================] - 3s 287ms/step - loss: 1.8104e-04 - val_loss: 0.0010
Epoch 95/100
12/12 [==============================] - 3s 277ms/step - loss: 1.3988e-04 - val_loss: 8.5343e-04
Epoch 96/100
12/12 [==============================] - 3s 280ms/step - loss: 1.4097e-04 - val_loss: 9.3255e-04
Epoch 97/100
12/12 [==============================] - 3s 287ms/step - loss: 1.4070e-04 - val_loss: 8.3848e-04
Epoch 98/100
12/12 [==============================] - 3s 290ms/step - loss: 1.3528e-04 - val_loss: 8.4349e-04
Epoch 99/100
12/12 [==============================] - 3s 288ms/step - loss: 1.4087e-04 - val_loss: 9.8092e-04
Epoch 100/100
12/12 [==============================] - 3s 285ms/step - loss: 1.4775e-04 - val_loss: 9.3230e-04
<tensorflow.python.keras.callbacks.History at 0x2d1aa544a58>
import tensorflow as tf
tf.__version__
'2.1.0'
### Lets Do the prediction and check performance metrics
train_predict=model.predict(X_train)
test_predict=model.predict(X_test)
##Transformback to original form
train_predict=scaler.inverse_transform(train_predict)
test_predict=scaler.inverse_transform(test_predict)
### Calculate RMSE performance metrics
import math
from sklearn.metrics import mean_squared_error
math.sqrt(mean_squared_error(y_train,train_predict))
140.9909210035748
### Test Data RMSE
math.sqrt(mean_squared_error(ytest,test_predict))
235.7193088627771
### Plotting 
# shift train predictions for plotting
look_back=100
trainPredictPlot = numpy.empty_like(df1)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(df1)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict
# plot baseline and predictions
plt.plot(scaler.inverse_transform(df1))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

len(test_data)
441
x_input=test_data[341:].reshape(1,-1)
x_input.shape
(1, 100)
temp_input=list(x_input)
temp_input=temp_input[0].tolist()
temp_input
[0.8583551465000423,
 0.8866418981676942,
 0.8743139407244789,
 0.8843198513890065,
 0.8783669678290975,
 0.8986321033521913,
 0.925821160179009,
 0.9287764924427933,
 0.9567677108840666,
 0.9386979650426415,
 0.933040614709111,
 0.9495060373216249,
 0.9642404796082076,
 0.9551211686228154,
 0.9598919192772104,
 0.9663514312251966,
 0.9624672802499368,
 0.9229502659799038,
 0.9598497002448705,
 0.9879253567508233,
 0.985941062230854,
 0.9253145317909315,
 0.9217259140420504,
 0.964747107996285,
 0.9757240564046274,
 0.9915984125643842,
 0.9697289538123788,
 0.9761462467280253,
 0.9679557544541082,
 1.0000000000000002,
 0.9901629654648318,
 0.9905007177235499,
 0.9653803934813816,
 0.9848855864223593,
 0.9708688676855528,
 0.9402600692392133,
 0.8774803681499621,
 0.8348391454867856,
 0.8541332432660644,
 0.7733682344000676,
 0.7726927298826314,
 0.8801401671873683,
 0.8400743054969182,
 0.8967322468969012,
 0.8552731571392387,
 0.8388499535590646,
 0.7423372456303303,
 0.8232711306256861,
 0.7814320695769654,
 0.6665963016127672,
 0.7921557037912694,
 0.6411804441442204,
 0.6861437135860848,
 0.6600101325677616,
 0.6520307354555435,
 0.5864223591995272,
 0.5658616904500551,
 0.660896732246897,
 0.6551549438486872,
 0.7097019336316812,
 0.664527569028118,
 0.6943764248923416,
 0.692181035210673,
 0.6356919699400492,
 0.6526640209406402,
 0.637802921557038,
 0.7267162036646122,
 0.7138816178333194,
 0.7419150553069325,
 0.7500211095161702,
 0.7722283205268936,
 0.8304905851557884,
 0.8194291986827664,
 0.8289706999915563,
 0.8125474964113824,
 0.7877649244279323,
 0.7516254327450818,
 0.7842607447437306,
 0.7797433082833742,
 0.8132652199611587,
 0.8141096006079542,
 0.7947310647639958,
 0.8333614793548934,
 0.8589884319851391,
 0.8390188296884238,
 0.8562864139153934,
 0.8748627881448958,
 0.887824031073208,
 0.9009541501308793,
 0.9279321117959978,
 0.9485349995778098,
 0.9333361479354896,
 0.9174617917757326,
 0.925441188887951,
 0.9177151059697712,
 0.9483239044161109,
 0.9406400405302711,
 0.9663514312251966,
 0.9563033015283293,
 0.964915984125644]
# demonstrate prediction for next 10 days
from numpy import array

lst_output=[]
n_steps=100
i=0
while(i<30):
    
    if(len(temp_input)>100):
        #print(temp_input)
        x_input=np.array(temp_input[1:])
        print("{} day input {}".format(i,x_input))
        x_input=x_input.reshape(1,-1)
        x_input = x_input.reshape((1, n_steps, 1))
        #print(x_input)
        yhat = model.predict(x_input, verbose=0)
        print("{} day output {}".format(i,yhat))
        temp_input.extend(yhat[0].tolist())
        temp_input=temp_input[1:]
        #print(temp_input)
        lst_output.extend(yhat.tolist())
        i=i+1
    else:
        x_input = x_input.reshape((1, n_steps,1))
        yhat = model.predict(x_input, verbose=0)
        print(yhat[0])
        temp_input.extend(yhat[0].tolist())
        print(len(temp_input))
        lst_output.extend(yhat.tolist())
        i=i+1
    

print(lst_output)
[0.94413203]
101
1 day input [0.8866419  0.87431394 0.88431985 0.87836697 0.8986321  0.92582116
 0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048
 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497
 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406
 0.99159841 0.96972895 0.97614625 0.96795575 1.         0.99016297
 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037
 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431
 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207
 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013 0.65203074
 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757
 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162
 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292
 0.8289707  0.8125475  0.78776492 0.75162543 0.78426074 0.77974331
 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883
 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535
 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004
 0.96635143 0.9563033  0.96491598 0.94413203]
1 day output [[0.9379593]]
2 day input [0.87431394 0.88431985 0.87836697 0.8986321  0.92582116 0.92877649
 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117
 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497  0.98792536
 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841
 0.96972895 0.97614625 0.96795575 1.         0.99016297 0.99050072
 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915
 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225
 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963
 0.7921557  0.64118044 0.68614371 0.66001013 0.65203074 0.58642236
 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642
 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162
 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707
 0.8125475  0.78776492 0.75162543 0.78426074 0.77974331 0.81326522
 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883 0.85628641
 0.87486279 0.88782403 0.90095415 0.92793211 0.948535   0.93333615
 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143
 0.9563033  0.96491598 0.94413203 0.93795931]
2 day output [[0.9286534]]
3 day input [0.88431985 0.87836697 0.8986321  0.92582116 0.92877649 0.95676771
 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192
 0.96635143 0.96246728 0.92295027 0.9598497  0.98792536 0.98594106
 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895
 0.97614625 0.96795575 1.         0.99016297 0.99050072 0.96538039
 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324
 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316
 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557
 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169
 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104
 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506
 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475
 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096
 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279
 0.88782403 0.90095415 0.92793211 0.948535   0.93333615 0.91746179
 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033
 0.96491598 0.94413203 0.93795931 0.92865342]
3 day output [[0.91987926]]
4 day input [0.87836697 0.8986321  0.92582116 0.92877649 0.95676771 0.93869797
 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143
 0.96246728 0.92295027 0.9598497  0.98792536 0.98594106 0.92531453
 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625
 0.96795575 1.         0.99016297 0.99050072 0.96538039 0.98488559
 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823
 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995
 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044
 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673
 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197
 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111
 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492
 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106
 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403
 0.90095415 0.92793211 0.948535   0.93333615 0.91746179 0.92544119
 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598
 0.94413203 0.93795931 0.92865342 0.91987926]
4 day output [[0.9128097]]
5 day input [0.8986321  0.92582116 0.92877649 0.95676771 0.93869797 0.93304061
 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728
 0.92295027 0.9598497  0.98792536 0.98594106 0.92531453 0.92172591
 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575
 1.         0.99016297 0.99050072 0.96538039 0.98488559 0.97086887
 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273
 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725
 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371
 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494
 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402
 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832
 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543
 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148
 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415
 0.92793211 0.948535   0.93333615 0.91746179 0.92544119 0.91771511
 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598 0.94413203
 0.93795931 0.92865342 0.91987926 0.91280973]
5 day output [[0.90777564]]
6 day input [0.92582116 0.92877649 0.95676771 0.93869797 0.93304061 0.94950604
 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027
 0.9598497  0.98792536 0.98594106 0.92531453 0.92172591 0.96474711
 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1.
 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007
 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017
 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113
 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013
 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193
 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292
 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832 0.83049059
 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543 0.78426074
 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843
 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211
 0.948535   0.93333615 0.91746179 0.92544119 0.91771511 0.9483239
 0.94064004 0.96635143 0.9563033  0.96491598 0.94413203 0.93795931
 0.92865342 0.91987926 0.91280973 0.90777564]
6 day output [[0.9047326]]
7 day input [0.92877649 0.95676771 0.93869797 0.93304061 0.94950604 0.96424048
 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497
 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406
 0.99159841 0.96972895 0.97614625 0.96795575 1.         0.99016297
 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037
 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431
 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207
 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013 0.65203074
 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757
 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162
 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292
 0.8289707  0.8125475  0.78776492 0.75162543 0.78426074 0.77974331
 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883
 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535
 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004
 0.96635143 0.9563033  0.96491598 0.94413203 0.93795931 0.92865342
 0.91987926 0.91280973 0.90777564 0.90473258]
7 day output [[0.9033923]]
8 day input [0.95676771 0.93869797 0.93304061 0.94950604 0.96424048 0.95512117
 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497  0.98792536
 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841
 0.96972895 0.97614625 0.96795575 1.         0.99016297 0.99050072
 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915
 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225
 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963
 0.7921557  0.64118044 0.68614371 0.66001013 0.65203074 0.58642236
 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642
 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162
 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707
 0.8125475  0.78776492 0.75162543 0.78426074 0.77974331 0.81326522
 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883 0.85628641
 0.87486279 0.88782403 0.90095415 0.92793211 0.948535   0.93333615
 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143
 0.9563033  0.96491598 0.94413203 0.93795931 0.92865342 0.91987926
 0.91280973 0.90777564 0.90473258 0.90339231]
8 day output [[0.90332204]]
9 day input [0.93869797 0.93304061 0.94950604 0.96424048 0.95512117 0.95989192
 0.96635143 0.96246728 0.92295027 0.9598497  0.98792536 0.98594106
 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895
 0.97614625 0.96795575 1.         0.99016297 0.99050072 0.96538039
 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324
 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316
 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557
 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169
 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104
 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506
 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475
 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096
 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279
 0.88782403 0.90095415 0.92793211 0.948535   0.93333615 0.91746179
 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033
 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973
 0.90777564 0.90473258 0.90339231 0.90332204]
9 day output [[0.9040391]]
10 day input [0.93304061 0.94950604 0.96424048 0.95512117 0.95989192 0.96635143
 0.96246728 0.92295027 0.9598497  0.98792536 0.98594106 0.92531453
 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625
 0.96795575 1.         0.99016297 0.99050072 0.96538039 0.98488559
 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823
 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995
 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044
 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673
 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197
 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111
 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492
 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106
 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403
 0.90095415 0.92793211 0.948535   0.93333615 0.91746179 0.92544119
 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598
 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564
 0.90473258 0.90339231 0.90332204 0.90403908]
10 day output [[0.9050924]]
11 day input [0.94950604 0.96424048 0.95512117 0.95989192 0.96635143 0.96246728
 0.92295027 0.9598497  0.98792536 0.98594106 0.92531453 0.92172591
 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575
 1.         0.99016297 0.99050072 0.96538039 0.98488559 0.97086887
 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273
 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725
 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371
 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494
 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402
 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832
 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543
 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148
 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415
 0.92793211 0.948535   0.93333615 0.91746179 0.92544119 0.91771511
 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598 0.94413203
 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258
 0.90339231 0.90332204 0.90403908 0.90509242]
11 day output [[0.906118]]
12 day input [0.96424048 0.95512117 0.95989192 0.96635143 0.96246728 0.92295027
 0.9598497  0.98792536 0.98594106 0.92531453 0.92172591 0.96474711
 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1.
 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007
 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017
 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113
 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013
 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193
 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292
 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832 0.83049059
 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543 0.78426074
 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843
 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211
 0.948535   0.93333615 0.91746179 0.92544119 0.91771511 0.9483239
 0.94064004 0.96635143 0.9563033  0.96491598 0.94413203 0.93795931
 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231
 0.90332204 0.90403908 0.90509242 0.90611798]
12 day output [[0.90686554]]
13 day input [0.95512117 0.95989192 0.96635143 0.96246728 0.92295027 0.9598497
 0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406
 0.99159841 0.96972895 0.97614625 0.96795575 1.         0.99016297
 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037
 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431
 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207
 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013 0.65203074
 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757
 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162
 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292
 0.8289707  0.8125475  0.78776492 0.75162543 0.78426074 0.77974331
 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883
 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535
 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004
 0.96635143 0.9563033  0.96491598 0.94413203 0.93795931 0.92865342
 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204
 0.90403908 0.90509242 0.90611798 0.90686554]
13 day output [[0.90720606]]
14 day input [0.95989192 0.96635143 0.96246728 0.92295027 0.9598497  0.98792536
 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841
 0.96972895 0.97614625 0.96795575 1.         0.99016297 0.99050072
 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915
 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225
 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963
 0.7921557  0.64118044 0.68614371 0.66001013 0.65203074 0.58642236
 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642
 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162
 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707
 0.8125475  0.78776492 0.75162543 0.78426074 0.77974331 0.81326522
 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883 0.85628641
 0.87486279 0.88782403 0.90095415 0.92793211 0.948535   0.93333615
 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143
 0.9563033  0.96491598 0.94413203 0.93795931 0.92865342 0.91987926
 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908
 0.90509242 0.90611798 0.90686554 0.90720606]
14 day output [[0.9071163]]
15 day input [0.96635143 0.96246728 0.92295027 0.9598497  0.98792536 0.98594106
 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895
 0.97614625 0.96795575 1.         0.99016297 0.99050072 0.96538039
 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324
 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316
 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557
 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169
 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104
 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506
 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475
 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096
 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279
 0.88782403 0.90095415 0.92793211 0.948535   0.93333615 0.91746179
 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033
 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973
 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242
 0.90611798 0.90686554 0.90720606 0.90711629]
15 day output [[0.9066538]]
16 day input [0.96246728 0.92295027 0.9598497  0.98792536 0.98594106 0.92531453
 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625
 0.96795575 1.         0.99016297 0.99050072 0.96538039 0.98488559
 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823
 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995
 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044
 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673
 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197
 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111
 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492
 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106
 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403
 0.90095415 0.92793211 0.948535   0.93333615 0.91746179 0.92544119
 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598
 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564
 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798
 0.90686554 0.90720606 0.90711629 0.90665382]
16 day output [[0.90592706]]
17 day input [0.92295027 0.9598497  0.98792536 0.98594106 0.92531453 0.92172591
 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575
 1.         0.99016297 0.99050072 0.96538039 0.98488559 0.97086887
 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273
 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725
 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371
 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494
 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402
 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832
 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543
 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148
 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415
 0.92793211 0.948535   0.93333615 0.91746179 0.92544119 0.91771511
 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598 0.94413203
 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258
 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554
 0.90720606 0.90711629 0.90665382 0.90592706]
17 day output [[0.9050646]]
18 day input [0.9598497  0.98792536 0.98594106 0.92531453 0.92172591 0.96474711
 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1.
 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007
 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017
 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113
 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013
 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193
 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292
 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832 0.83049059
 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543 0.78426074
 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843
 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211
 0.948535   0.93333615 0.91746179 0.92544119 0.91771511 0.9483239
 0.94064004 0.96635143 0.9563033  0.96491598 0.94413203 0.93795931
 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231
 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606
 0.90711629 0.90665382 0.90592706 0.90506458]
18 day output [[0.90419257]]
19 day input [0.98792536 0.98594106 0.92531453 0.92172591 0.96474711 0.97572406
 0.99159841 0.96972895 0.97614625 0.96795575 1.         0.99016297
 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037
 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431
 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207
 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013 0.65203074
 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757
 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162
 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292
 0.8289707  0.8125475  0.78776492 0.75162543 0.78426074 0.77974331
 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883
 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535
 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004
 0.96635143 0.9563033  0.96491598 0.94413203 0.93795931 0.92865342
 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204
 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629
 0.90665382 0.90592706 0.90506458 0.90419257]
19 day output [[0.9034131]]
20 day input [0.98594106 0.92531453 0.92172591 0.96474711 0.97572406 0.99159841
 0.96972895 0.97614625 0.96795575 1.         0.99016297 0.99050072
 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915
 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225
 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963
 0.7921557  0.64118044 0.68614371 0.66001013 0.65203074 0.58642236
 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642
 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162
 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707
 0.8125475  0.78776492 0.75162543 0.78426074 0.77974331 0.81326522
 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883 0.85628641
 0.87486279 0.88782403 0.90095415 0.92793211 0.948535   0.93333615
 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143
 0.9563033  0.96491598 0.94413203 0.93795931 0.92865342 0.91987926
 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908
 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382
 0.90592706 0.90506458 0.90419257 0.90341312]
20 day output [[0.90279734]]
21 day input [0.92531453 0.92172591 0.96474711 0.97572406 0.99159841 0.96972895
 0.97614625 0.96795575 1.         0.99016297 0.99050072 0.96538039
 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324
 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316
 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557
 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169
 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104
 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506
 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475
 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096
 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279
 0.88782403 0.90095415 0.92793211 0.948535   0.93333615 0.91746179
 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033
 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973
 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242
 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706
 0.90506458 0.90419257 0.90341312 0.90279734]
21 day output [[0.9023812]]
22 day input [0.92172591 0.96474711 0.97572406 0.99159841 0.96972895 0.97614625
 0.96795575 1.         0.99016297 0.99050072 0.96538039 0.98488559
 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823
 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995
 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044
 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673
 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197
 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111
 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492
 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106
 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403
 0.90095415 0.92793211 0.948535   0.93333615 0.91746179 0.92544119
 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598
 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564
 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798
 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458
 0.90419257 0.90341312 0.90279734 0.90238118]
22 day output [[0.9021694]]
23 day input [0.96474711 0.97572406 0.99159841 0.96972895 0.97614625 0.96795575
 1.         0.99016297 0.99050072 0.96538039 0.98488559 0.97086887
 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273
 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725
 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371
 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494
 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402
 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832
 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543
 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148
 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415
 0.92793211 0.948535   0.93333615 0.91746179 0.92544119 0.91771511
 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598 0.94413203
 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258
 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554
 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257
 0.90341312 0.90279734 0.90238118 0.90216941]
23 day output [[0.90213937]]
24 day input [0.97572406 0.99159841 0.96972895 0.97614625 0.96795575 1.
 0.99016297 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007
 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017
 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113
 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013
 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193
 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292
 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832 0.83049059
 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543 0.78426074
 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843
 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211
 0.948535   0.93333615 0.91746179 0.92544119 0.91771511 0.9483239
 0.94064004 0.96635143 0.9563033  0.96491598 0.94413203 0.93795931
 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231
 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606
 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312
 0.90279734 0.90238118 0.90216941 0.90213937]
24 day output [[0.9022528]]
25 day input [0.99159841 0.96972895 0.97614625 0.96795575 1.         0.99016297
 0.99050072 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037
 0.83483915 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431
 0.89673225 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207
 0.6665963  0.7921557  0.64118044 0.68614371 0.66001013 0.65203074
 0.58642236 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757
 0.69437642 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162
 0.71388162 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292
 0.8289707  0.8125475  0.78776492 0.75162543 0.78426074 0.77974331
 0.81326522 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883
 0.85628641 0.87486279 0.88782403 0.90095415 0.92793211 0.948535
 0.93333615 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004
 0.96635143 0.9563033  0.96491598 0.94413203 0.93795931 0.92865342
 0.91987926 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204
 0.90403908 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629
 0.90665382 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734
 0.90238118 0.90216941 0.90213937 0.90225279]
25 day output [[0.90246403]]
26 day input [0.96972895 0.97614625 0.96795575 1.         0.99016297 0.99050072
 0.96538039 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915
 0.85413324 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225
 0.85527316 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963
 0.7921557  0.64118044 0.68614371 0.66001013 0.65203074 0.58642236
 0.56586169 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642
 0.69218104 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162
 0.74191506 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707
 0.8125475  0.78776492 0.75162543 0.78426074 0.77974331 0.81326522
 0.8141096  0.79473106 0.83336148 0.85898843 0.83901883 0.85628641
 0.87486279 0.88782403 0.90095415 0.92793211 0.948535   0.93333615
 0.91746179 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143
 0.9563033  0.96491598 0.94413203 0.93795931 0.92865342 0.91987926
 0.91280973 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908
 0.90509242 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382
 0.90592706 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118
 0.90216941 0.90213937 0.90225279 0.90246403]
26 day output [[0.90272856]]
27 day input [0.97614625 0.96795575 1.         0.99016297 0.99050072 0.96538039
 0.98488559 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324
 0.77336823 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316
 0.83884995 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557
 0.64118044 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169
 0.66089673 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104
 0.63569197 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506
 0.75002111 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475
 0.78776492 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096
 0.79473106 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279
 0.88782403 0.90095415 0.92793211 0.948535   0.93333615 0.91746179
 0.92544119 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033
 0.96491598 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973
 0.90777564 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242
 0.90611798 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706
 0.90506458 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941
 0.90213937 0.90225279 0.90246403 0.90272856]
27 day output [[0.90300757]]
28 day input [0.96795575 1.         0.99016297 0.99050072 0.96538039 0.98488559
 0.97086887 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823
 0.77269273 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995
 0.74233725 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044
 0.68614371 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673
 0.65515494 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197
 0.65266402 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111
 0.77222832 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492
 0.75162543 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106
 0.83336148 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403
 0.90095415 0.92793211 0.948535   0.93333615 0.91746179 0.92544119
 0.91771511 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598
 0.94413203 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564
 0.90473258 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798
 0.90686554 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458
 0.90419257 0.90341312 0.90279734 0.90238118 0.90216941 0.90213937
 0.90225279 0.90246403 0.90272856 0.90300757]
28 day output [[0.903272]]
29 day input [1.         0.99016297 0.99050072 0.96538039 0.98488559 0.97086887
 0.94026007 0.87748037 0.83483915 0.85413324 0.77336823 0.77269273
 0.88014017 0.84007431 0.89673225 0.85527316 0.83884995 0.74233725
 0.82327113 0.78143207 0.6665963  0.7921557  0.64118044 0.68614371
 0.66001013 0.65203074 0.58642236 0.56586169 0.66089673 0.65515494
 0.70970193 0.66452757 0.69437642 0.69218104 0.63569197 0.65266402
 0.63780292 0.7267162  0.71388162 0.74191506 0.75002111 0.77222832
 0.83049059 0.8194292  0.8289707  0.8125475  0.78776492 0.75162543
 0.78426074 0.77974331 0.81326522 0.8141096  0.79473106 0.83336148
 0.85898843 0.83901883 0.85628641 0.87486279 0.88782403 0.90095415
 0.92793211 0.948535   0.93333615 0.91746179 0.92544119 0.91771511
 0.9483239  0.94064004 0.96635143 0.9563033  0.96491598 0.94413203
 0.93795931 0.92865342 0.91987926 0.91280973 0.90777564 0.90473258
 0.90339231 0.90332204 0.90403908 0.90509242 0.90611798 0.90686554
 0.90720606 0.90711629 0.90665382 0.90592706 0.90506458 0.90419257
 0.90341312 0.90279734 0.90238118 0.90216941 0.90213937 0.90225279
 0.90246403 0.90272856 0.90300757 0.90327197]
29 day output [[0.90350425]]
[[0.9441320300102234], [0.9379593133926392], [0.9286534190177917], [0.9198792576789856], [0.9128097295761108], [0.9077756404876709], [0.9047325849533081], [0.9033923149108887], [0.9033220410346985], [0.9040390849113464], [0.9050924181938171], [0.9061179757118225], [0.9068655371665955], [0.9072060585021973], [0.9071162939071655], [0.9066538214683533], [0.9059270620346069], [0.905064582824707], [0.9041925668716431], [0.9034131169319153], [0.9027973413467407], [0.902381181716919], [0.902169406414032], [0.9021393656730652], [0.9022527933120728], [0.9024640321731567], [0.9027285575866699], [0.9030075669288635], [0.9032719731330872], [0.9035042524337769]]
day_new=np.arange(1,101)
day_pred=np.arange(101,131)
import matplotlib.pyplot as plt
len(df1)
1258
plt.plot(day_new,scaler.inverse_transform(df1[1158:]))
plt.plot(day_pred,scaler.inverse_transform(lst_output))
[<matplotlib.lines.Line2D at 0x2d1b0f352b0>]

df3=df1.tolist()
df3.extend(lst_output)
plt.plot(df3[1200:])
[<matplotlib.lines.Line2D at 0x2d1b0f55ac8>]

df3=scaler.inverse_transform(df3).tolist()
plt.plot(df3)
[<matplotlib.lines.Line2D at 0x2d1a904c470>]
THANKYOU!!!
